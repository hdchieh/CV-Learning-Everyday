<html>
<head>
  <title>Evernote Export</title>
  <basefont face="微软雅黑" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="YXBJ Windows/600753 (zh-CN, DDL); Windows/10.0.0 (Win64);"/>
  <meta name="content-class" content="maxiang"/>
  <style>
    body, td {
      font-family: 微软雅黑;
      font-size: 10pt;
    }
  </style>
</head>
<body>
<a name="1011"/>

<div><span lang="v2" style="color: #2c3f51; line-height: 1.6; background: none;"><del style="position:relative;display:block;z-index:10;"><a href="http://maxiang.info/#/?provider=evernote&amp;guid=68d48ad7-a547-4b4d-93d3-24d2f808697c&amp;notebook=CV%20Learning" style="position: absolute;color: #FFF;text-decoration: none;font-size: 12px;height: 25px;border-radius: 0;margin-top: -20px;right: 15px;background: rgba(0, 0, 0, 0);border-left: 10px solid #BB3A34;border-right: 10px solid #BB3A34;border-bottom: 5px solid rgba(0, 0, 0, 0);width: 0;text-indent:-100000px;">Edit</a></del><div style="color: #2c3f51; line-height: 1.6;">
                        
                    



<h1 style="font-size: 41.6px; margin: 1.2em 0 .6em 0; font-family: inherit; font-weight: bold; line-height: 1.1; color: inherit; margin-top: 21px; margin-bottom: 10.5px; text-align: start;">Visual Tracker Roadmap</h1>

<p style="margin: 0 0 1.1em; line-height: 1.6;"></p>

<blockquote style="padding: 15px 20px; margin: 0 0 1.1em; border-left: 5px solid rgba(102,128,153,0.075); border-left-width: 10px; background-color: rgba(102,128,153,0.05); border-top-right-radius: 5px; border-bottom-right-radius: 5px;">
  <p style="margin: 0 0 1.1em; font-size: 1em; font-weight: 300; margin-bottom: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">作者：<a href="https://github.com/huuuuusy" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">huuuuusy</a></strong> <br/>
  <strong style="font-weight: bold; line-height: 1.6;">修改日期：2019-09-05</strong></p>
</blockquote>

<p style="margin: 0 0 1.1em; line-height: 1.6;"><img src="YinXiangBiJi.enex_files/httpsuser-images.githubusercontent.com3211790662909573-b380cd00-bdaf-11e9-8ce0-495777184f02.png" type="image/png" data-filename="httpsuser-images.githubusercontent.com3211790662909573-b380cd00-bdaf-11e9-8ce0-495777184f02.png" alt="image" longdesc="https://user-images.githubusercontent.com/32117906/62909573-b380cd00-bdaf-11e9-8ce0-495777184f02.png" style="border: 0; vertical-align: middle; max-width: 100%;" title=""/> <br/>
图源： <a href="https://github.com/foolwood" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">王强博士</a></p>

<p style="margin: 0 0 1.1em; line-height: 1.6;">本文基于王强博士的roadmap并添加相关论文的链接。</p>

<h1 style="font-size: 41.6px; margin: 1.2em 0 .6em 0; font-family: inherit; font-weight: bold; line-height: 1.1; color: inherit; margin-top: 21px; margin-bottom: 10.5px; text-align: start;">1 Benchmark</h1>

<h2 style="font-family: inherit; font-weight: bold; line-height: 1.1; color: inherit; margin-top: 21px; margin-bottom: 10.5px; font-size: 34.4px; margin: 1.2em 0 .6em 0; text-align: start;">1.1 经典</h2>

<ul style="margin-top: 0; margin-bottom: 1.1em; padding-left: 2em; line-height: 1.6;"><li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">OBT</strong></p>

<ul style="margin-top: 0; padding-left: 2em; margin-bottom: .55em; line-height: 1.6;">
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">OTB100</strong>【TPAMI’15】 <a href="https://ieeexplore.ieee.org/document/7001050" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Object tracking benchmark】 </a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">OTB50</strong>【CVPR’13】<a href="http://faculty.ucmerced.edu/mhyang/papers/cvpr13_benchmark.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Online object tracking: A benchmark】</a></p></li></ul></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><a href="http://votchallenge.net/" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank"><strong style="font-weight: bold; line-height: 1.6;">VOT</strong></a></p>

<ul style="margin-top: 0; padding-left: 2em; margin-bottom: .55em; line-height: 1.6;">
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><a href="http://votchallenge.net/vot2019/" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank"><strong style="font-weight: bold; line-height: 1.6;">VOT2019 Challenge</strong></a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><a href="http://votchallenge.net/vot2018/" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank"><strong style="font-weight: bold; line-height: 1.6;">VOT2018</strong></a>【ECCV’18】<a href="https://prints.vicos.si/publications/365/the-sixth-visual-object-tracking-vot2018-challenge-results" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【The sixth Visual Object Tracking VOT2018 challenge results】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><a href="http://votchallenge.net/vot2017/" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank"><strong style="font-weight: bold; line-height: 1.6;">VOT2017</strong></a>【ICCV’17】 <a href="https://prints.vicos.si/publications/359/the-visual-object-tracking-vot2017-challenge-results" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【The Visual Object Tracking VOT2017 Challenge Results】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><a href="http://votchallenge.net/vot2016/" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank"><strong style="font-weight: bold; line-height: 1.6;">VOT2016</strong></a>【ECCV’16】<a href="https://prints.vicos.si/publications/340/the-visual-object-tracking-vot2016-challenge-results" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【The Visual Object Tracking VOT2016 challenge results】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><a href="http://votchallenge.net/vot2015/" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank"><strong style="font-weight: bold; line-height: 1.6;">VOT2015</strong></a>【ICCV’15】<a href="https://prints.vicos.si/publications/325/the-visual-object-tracking-vot2015-challenge-results" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【The Visual Object Tracking VOT2015 challenge results】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><a href="http://votchallenge.net/vot2014/" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank"><strong style="font-weight: bold; line-height: 1.6;">VOT2014</strong></a>【ECCV’14】<a href="https://prints.vicos.si/publications/315/the-visual-object-tracking-vot2014-challenge-results" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【The Visual Object Tracking VOT2014 challenge results】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><a href="http://votchallenge.net/vot2013/" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank"><strong style="font-weight: bold; line-height: 1.6;">VOT2013</strong></a>【ICCV’13】<a href="https://prints.vicos.si/publications/304/the-visual-object-tracking-vot2013-challenge-results" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【The Visual Object Tracking VOT2013 challenge results】</a></p></li></ul></li>
</ul>

<h2 style="font-family: inherit; font-weight: bold; line-height: 1.1; color: inherit; margin-top: 21px; margin-bottom: 10.5px; font-size: 34.4px; margin: 1.2em 0 .6em 0; text-align: start;">1.2 其他</h2>

<ul style="margin-top: 0; margin-bottom: 1.1em; padding-left: 2em; line-height: 1.6;"><li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">GOT-10K</strong>【2018】 <a href="https://arxiv.org/abs/1810.11981" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GOT-10k: A Large High-Diversity Benchmark for Generic Object Tracking in the Wild】</a><a href="https://github.com/got-10k/toolkit" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】</a><a href="http://got-10k.aitestunion.com/" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">LaSOT</strong>【CVPR’19】<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Fan_LaSOT_A_High-Quality_Benchmark_for_Large-Scale_Single_Object_Tracking_CVPR_2019_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【LaSOT: A High-quality Benchmark for Large-scale Single Object Tracking】</a><a href="https://cis.temple.edu/lasot/" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">OxUvA</strong>【ECCV’18】<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Efstratios_Gavves_Long-term_Tracking_in_ECCV_2018_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Long-term Tracking in the Wild: A Benchmark】</a><a href="https://oxuva.github.io/long-term-tracking-benchmark/" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】</a><a href="https://github.com/oxuva/long-term-tracking-benchmark#oxuva-long-term-tracking-benchmark-eccv18" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">TrackingNet</strong>【ECCV’18】<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Matthias_Muller_TrackingNet_A_Large-Scale_ECCV_2018_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【TrackingNet: A Large-Scale Dataset and Benchmark for Object Tracking in the Wild】</a> <a href="https://tracking-net.org/" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】</a> <a href="https://github.com/SilvioGiancola/TrackingNet-devkit" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">UAVDT</strong>【ECCV’18】<a href="https://arxiv.org/pdf/1804.00518.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【The Unmanned Aerial Vehicle Benchmark: Object Detection and Tracking】</a><a href="https://sites.google.com/site/daviddo0323/projects/uavdt" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">AMP</strong>【ICCV’17】<a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Zajc_Beyond_Standard_Benchmarks_ICCV_2017_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Beyond Standard Benchmarks: Parameterizing Performance Evaluation in Visual Object Tracking】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">Nfs</strong>【ICCV’17】<a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Galoogahi_Need_for_Speed_ICCV_2017_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Need for Speed: A Benchmark for Higher Frame Rate Object Tracking】</a><a href="http://ci2cv.net/nfs/index.html" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">DTB70</strong>【AAAI’17】<a href="http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14338/14292" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Visual Object Tracking for Unmanned Aerial Vehicles: A Benchmark and New Motion Models】</a><a href="https://github.com/flyers/drone-tracking" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">UAV123</strong>【ECCV’16】<a href="https://ivul.kaust.edu.sa/Documents/Publications/2016/A%20Benchmark%20and%20Simulator%20for%20UAV%20Tracking.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【A Benchmark and Simulator for UAV Tracking】</a> <a href="https://ivul.kaust.edu.sa/Pages/pub-benchmark-simulator-uav.aspx" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">TColor-128</strong>【TIP’15】<a href="http://www.dabi.temple.edu/~hbling/publication/TColor-128.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Encoding color information for visual tracking: Algorithms and benchmark】</a><a href="http://www.dabi.temple.edu/~hbling/data/TColor-128/TColor-128.html" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">NUS-PRO</strong>【PAMI’15】 <a href="http://faculty.ucmerced.edu/mhyang/papers/pami15_nus_pro.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【NUS-PRO: A New Visual Tracking Challenge】</a><a href="https://sites.google.com/site/li00annan/nus-pro" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">ALOV300+</strong>【PAMI’14】<a href="http://crcv.ucf.edu/papers/Tracking_Survey.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Visual Tracking: An Experimental Survey】</a><a href="http://imagelab.ing.unimore.it/dsm/" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">PTB</strong>【ICCV’13】<a href="http://vision.princeton.edu/projects/2013/tracking/paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Tracking Revisited using RGBD Camera: Unified Benchmark and Baselines】</a> <a href="http://tracking.cs.princeton.edu/" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】</a></p></li>
</ul>



<h2 style="font-family: inherit; font-weight: bold; line-height: 1.1; color: inherit; margin-top: 21px; margin-bottom: 10.5px; font-size: 34.4px; margin: 1.2em 0 .6em 0; text-align: start;">2 数据库</h2>

<h3 style="font-family: inherit; font-weight: bold; color: inherit; margin-top: 21px; margin-bottom: 10.5px; font-size: 27.2px; margin: 1.2em 0 .6em 0; text-align: start; line-height: 1.6;">2.1 <a href="https://davischallenge.org/" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">DAVIS</a></h3>

<ul style="margin-top: 0; margin-bottom: 1.1em; padding-left: 2em; line-height: 1.6;"><li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;">Datasets</p>

<ul style="margin-top: 0; padding-left: 2em; margin-bottom: .55em; line-height: 1.6;">
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><a href="https://davischallenge.org/davis2017/code.html" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">DAVIS2017</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><a href="https://davischallenge.org/davis2016/code.html" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">DAVIS2016</a></p></li></ul></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;">Publications</p>

<ul style="margin-top: 0; padding-left: 2em; margin-bottom: .55em; line-height: 1.6;">
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><a href="https://arxiv.org/pdf/1905.00737" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">The 2019 DAVIS Challenge on VOS: Unsupervised Multi-Object Segmentation</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><a href="https://arxiv.org/abs/1803.00557" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">The 2018 DAVIS Challenge on Video Object Segmentation</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><a href="https://arxiv.org/abs/1704.00675" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">The 2017 DAVIS Challenge on Video Object Segmentation</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Perazzi_A_Benchmark_Dataset_CVPR_2016_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【CVPR’16】A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation</a></p></li></ul></li>
</ul>

<h3 style="font-family: inherit; font-weight: bold; color: inherit; margin-top: 21px; margin-bottom: 10.5px; font-size: 27.2px; margin: 1.2em 0 .6em 0; text-align: start; line-height: 1.6;">2.2 <a href="http://image-net.org/challenges/LSVRC" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">ILSVRC</a></h3>

<ul style="margin-top: 0; margin-bottom: 1.1em; padding-left: 2em; line-height: 1.6;"><li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><a href="http://image-net.org/challenges/LSVRC/2017/" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">ILSVRC2017-VID</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><a href="http://image-net.org/challenges/LSVRC/2016/" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">ILSVRC2016-VID</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><a href="http://image-net.org/challenges/LSVRC/2015/" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">ILSVRC2015-VID</a></p></li>
</ul>

<h3 style="font-family: inherit; font-weight: bold; color: inherit; margin-top: 21px; margin-bottom: 10.5px; font-size: 27.2px; margin: 1.2em 0 .6em 0; text-align: start; line-height: 1.6;">2.3 YouTube BoundingBox</h3>

<ul style="margin-top: 0; margin-bottom: 1.1em; padding-left: 2em; line-height: 1.6;"><li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Real_YouTube-BoundingBoxes_A_Large_CVPR_2017_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【CVPR’17】YouTube-BoundingBoxes: A Large High-Precision Human-Annotated Data Set for Object Detection in Video</a></p>

<ul style="margin-top: 0; padding-left: 2em; margin-bottom: .55em; line-height: 1.6;">
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><a href="https://github.com/mbuckler/youtube-bb" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">GitHub</a></p></li></ul></li>
</ul>

<h2 style="font-family: inherit; font-weight: bold; line-height: 1.1; color: inherit; margin-top: 21px; margin-bottom: 10.5px; font-size: 34.4px; margin: 1.2em 0 .6em 0; text-align: start;">3 相关滤波</h2>

<p style="margin: 0 0 1.1em; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">MOSSE</strong>【CVPR’10】<a href="http://www.cs.colostate.edu/~draper/papers/bolme_cvpr10.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Visual object tracking using adaptive correlation filters】</a></p>

<ul style="margin-top: 0; margin-bottom: 1.1em; padding-left: 2em; line-height: 1.6;"><li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">MCCF</strong>【ICCV’13】<a href="https://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Galoogahi_Multi-channel_Correlation_Filters_2013_ICCV_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Multi-Channel Correlation Filters】</a><a href="http://www.hamedkiani.com/mccf.html" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】</a></p>

<ul style="margin-top: 0; padding-left: 2em; margin-bottom: .55em; line-height: 1.6;">
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">CFLB</strong>【ICCV’15】<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Galoogahi_Correlation_Filters_With_2015_CVPR_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Correlation Filters with Limited Boundaries】</a><a href="http://www.hamedkiani.com/cfwlb.html" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】</a></p>

<ul style="margin-top: 0; padding-left: 2em; margin-bottom: .55em; line-height: 1.6;">
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">BACF</strong>【ICCV’17】<a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Galoogahi_Learning_Background-Aware_Correlation_ICCV_2017_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Learning Background-Aware Correlation Filters for Visual Tracking】</a><a href="http://www.hamedkiani.com/bacf.html" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】</a></p></li></ul></li></ul></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">CSK</strong>【ECCV’12】<a href="http://www.robots.ox.ac.uk/~joao/publications/henriques_eccv2012.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Exploiting the circulant structure of tracking-by-detection with kernels】</a></p>

<ul style="margin-top: 0; padding-left: 2em; margin-bottom: .55em; line-height: 1.6;">
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">CN</strong> 【CVPR’14】<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Danelljan_Adaptive_Color_Attributes_2014_CVPR_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Adaptive color attributes for real-time visual tracking】</a><a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/colvistrack/index.html" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">STC</strong>【ECCV’14】<a href="https://arxiv.org/pdf/1311.1939v1.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Fast Tracking via Spatio-Temporal Context Learning】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">SAMF</strong>【ECCVW’14】<a href="http://github.com/ihpdep/ihpdep.github.io/raw/master/papers/eccvw14_samf.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【A Scale Adaptive Kernel Correlation Filter Tracker with Feature Integration】</a><a href="https://github.com/ihpdep/samf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">DSST</strong>【BMVC’14】 <a href="http://www.bmva.org/bmvc/2014/files/paper038.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Accurate Scale Estimation for Robust Visual Tracking】</a>【TPAMI’17】 <a href="https://arxiv.org/abs/1609.06141v1" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Paper】</a><a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/scalvistrack/index.html" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">KCF</strong> 【TPAMI’15】<a href="https://arxiv.org/abs/1404.7584" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【High-Speed Tracking with Kernelized Correlation Filters】</a> <a href="https://github.com/foolwood/KCF" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】</a></p>

<ul style="margin-top: 0; padding-left: 2em; margin-bottom: .55em; line-height: 1.6;">
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">CACF</strong>【CVPR’17 Oral】<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Mueller_Context-Aware_Correlation_Filter_CVPR_2017_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Context-Aware Correlation Filter Tracking】</a><a href="https://github.com/thias15/Context-Aware-CF-Tracking" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">Staple</strong>【CVPR’16】<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Bertinetto_Staple_Complementary_Learners_CVPR_2016_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Staple: Complementary Learners for Real-Time Tracking】</a> <a href="http://www.robots.ox.ac.uk/~luca/staple.html" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】</a><a href="https://github.com/bertinetto/staple" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">RAJSSC</strong>【ICCVW’15】<a href="http://www.cv-foundation.org//openaccess/content_iccv_2015_workshops/w14/papers/Zhang_Joint_Scale-Spatial_Correlation_ICCV_2015_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Joint Scale-Spatial Correlation Tracking with Adaptive Rotation Estimation】</a><a href="http://www.votchallenge.net/vot2015/download/poster_Mengdan_Zhang.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Poster】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">KCF_MTS</strong>【ICCVW’15】<a href="https://www.cv-foundation.org/openaccess/content_iccv_2015_workshops/w14/papers/Bibi_Multi-Template_Scale-Adaptive_Kernelized_ICCV_2015_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Multi-Template Scale-Adaptive Kernelized Correlation Filters】</a><a href="https://github.com/adelbibi/Multi-Template-Scale-Adaptive-Kernelized-Correlation-Filters" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">LCT</strong>【CVPR’15】<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ma_Long-Term_Correlation_Tracking_2015_CVPR_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Long-term Correlation Tracking】</a>【IJCV’18】<a href="http://faculty.ucmerced.edu/mhyang/papers/ijcv18_lct.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Paper】</a><a href="https://github.com/chaoma99/lct-tracker" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">MUSTer</strong>【CVPR’15】<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Hong_MUlti-Store_Tracker_MUSTer_2015_CVPR_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【MUlti-Store Tracker (MUSTer): A Cognitive Psychology Inspired Approach to Object Tracking】</a><a href="https://sites.google.com/site/zhibinhong4131/Projects/muster" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">CF2</strong>【ICCV’15】<a href="http://faculty.ucmerced.edu/mhyang/papers/iccv15_tracking.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Hierarchical Convolutional Features for Visual Tracking】</a><a href="https://sites.google.com/site/jbhuang0604/publications/cf2" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】 </a> <a href="https://github.com/jbhuang0604/CF2" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】</a></p>

<ul style="margin-top: 0; padding-left: 2em; margin-bottom: .55em; line-height: 1.6;">
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">HDT</strong>【CVPR’16】<a href="http://faculty.ucmerced.edu/mhyang/papers/cvpr16_hedge_tracking.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Hedged Deep Tracking】</a><a href="https://sites.google.com/site/yuankiqi/hdt/" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">MCPF</strong>【TPAMI’17】<a href="https://ieeexplore.ieee.org/document/8267285/" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Learning Multi-task Correlation Particle Filters for Visual Tracking】</a><a href="http://nlpr-web.ia.ac.cn/mmc/homepage/tzzhang/lmcpf.html" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】 </a><a href="http://nlpr-web.ia.ac.cn/mmc/homepage/tzzhang/Project_Tianzhu/zhang_mcpf/Source_Code/Source_Code.zip" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Code】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">IBCCF</strong>【ICCVW’17】<a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w28/Li_Integrating_Boundary_and_ICCV_2017_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Integrating Boundary and Center Correlation Filters for Visual Tracking With Aspect Ratio Variation】</a><a href="https://github.com/lifeng9472/IBCCF" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】</a></p></li></ul></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">SRDCF</strong>【ICCV’15】<a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Danelljan_Learning_Spatially_Regularized_ICCV_2015_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Learning Spatially Regularized Correlation Filters for Visual Tracking】</a><a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/regvistrack/index.html" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】 </a></p>

<ul style="margin-top: 0; padding-left: 2em; margin-bottom: .55em; line-height: 1.6;">
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">STRCF</strong> 【CVPR’18】<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Learning_Spatial-Temporal_Regularized_CVPR_2018_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Learning Spatial-Temporal Regularized Correlation Filters for Visual Tracking】</a><a href="https://github.com/lifeng9472/STRCF" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】 </a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">CSR-DCF</strong>【CVPR’17】<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Lukezic_Discriminative_Correlation_Filter_CVPR_2017_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Discriminative Correlation Filter with Channel and Spatial Reliability】</a><a href="http://openaccess.thecvf.com/content_cvpr_2017/supplemental/Lukezic_Discriminative_Correlation_Filter_2017_CVPR_supplemental.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Supp】</a> <a href="https://github.com/alanlukezic/csr-dcf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】 </a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">SRDCFdecon</strong> 【CVPR’16】<a href="https://www.cvl.isy.liu.se/research/objrec/visualtracking/decontrack/AdaptiveDecon_CVPR16.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Adaptive Decontamination of the Training Set: A Unified Formulation for Discriminative Visual Tracking】</a> <a href="https://www.cvl.isy.liu.se/research/objrec/visualtracking/decontrack/index.html" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】 </a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">DeepSRDCF</strong>  【ICCV’15】<a href="https://www.cvl.isy.liu.se/research/objrec/visualtracking/regvistrack/ConvDCF_ICCV15_VOTworkshop.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Convolutional Features for Correlation Filter Based Visual Tracking】</a> <a href="https://www.cvl.isy.liu.se/research/objrec/visualtracking/regvistrack/" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】 </a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">DMSRDCF</strong>【ICPR’16 Best Paper】<a href="https://arxiv.org/pdf/1612.06615v1.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Deep Motion Features for Visual Tracking】</a> </p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">C-COT</strong>【ECCV’16】<a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/conttrack/C-COT_ECCV16.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Beyond Correlation Filters: Learning Continuous Convolution Operators for Visual Trackin】</a> <a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/conttrack/index.html" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】 </a><a href="https://github.com/martin-danelljan/Continuous-ConvOp" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】 </a></p>

<ul style="margin-top: 0; padding-left: 2em; margin-bottom: .55em; line-height: 1.6;">
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">CFCF</strong>【TIP’18】<a href="https://www.researchgate.net/publication/316428659_Good_Features_to_Correlate_for_Visual_Tracking" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Good Features to Correlate for Visual Tracking】</a> <a href="https://github.com/egundogdu/CFCF" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】 </a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">ECO</strong>【CVPR’17】<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Danelljan_ECO_Efficient_Convolution_CVPR_2017_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【ECO: Efficient Convolution Operators for Tracking】</a> <a href="http://openaccess.thecvf.com/content_cvpr_2017/supplemental/Danelljan_ECO_Efficient_Convolution_2017_CVPR_supplemental.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Supp】</a><a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/ecotrack/index.html" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】 </a><a href="https://github.com/martin-danelljan/ECO" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】 </a></p>

<ul style="margin-top: 0; padding-left: 2em; margin-bottom: .55em; line-height: 1.6;">
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">UPDT</strong>【ECCV’18】<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Goutam_Bhat_Unveiling_the_Power_ECCV_2018_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Unveiling the Power of Deep Tracking】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">CFWCR</strong>【ICCVW’17】<a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w28/He_Correlation_Filters_With_ICCV_2017_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Correlation Filters With Weighted Convolution Responses】</a> <a href="https://github.com/he010103/CFWCR" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】 </a></p></li></ul></li></ul></li></ul></li></ul></li></ul></li>
</ul>



<h2 style="font-family: inherit; font-weight: bold; line-height: 1.1; color: inherit; margin-top: 21px; margin-bottom: 10.5px; font-size: 34.4px; margin: 1.2em 0 .6em 0; text-align: start;">4 深度学习</h2>

<ul style="margin-top: 0; margin-bottom: 1.1em; padding-left: 2em; line-height: 1.6;"><li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">ATOM</strong>【CVPR’19 Oral】<a href="https://arxiv.org/pdf/1811.07628.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【ATOM: Accurate Tracking by Overlap Maximization】</a><a href="https://github.com/visionml/pytracking" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">DLT</strong>【NIPS’13】<a href="http://winsty.net/papers/dlt.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Learning A Deep Compact Image Representation for Visual Tracking】</a><a href="http://winsty.net/dlt.html" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】</a></p>

<ul style="margin-top: 0; padding-left: 2em; margin-bottom: .55em; line-height: 1.6;">
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">SO-DLT</strong>【2015】<a href="https://arxiv.org/pdf/1501.04587v2.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Transferring Rich Feature Hierarchies for Robust Visual Tracking】</a><a href="http://www.votchallenge.net/vot2016/download/08_SO-DLT.zip" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Code】</a></p></li></ul></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">FCNT</strong>【ICCV’15】<a href="http://202.118.75.4/lu/Paper/ICCV2015/iccv15_lijun.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Visual Tracking with Fully Convolutional Networks】</a><a href="https://github.com/scott89/FCNT" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】</a><a href="http://scott89.github.io/FCNT/" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】</a></p>

<ul style="margin-top: 0; padding-left: 2em; margin-bottom: .55em; line-height: 1.6;">
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">STCT</strong>【CVPR’16】<a href="http://www.ee.cuhk.edu.hk/~wlouyang/Papers/WangLJ_CVPR16.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【STCT: Sequentially Training Convolutional Networks for Visual Tracking】</a><a href="https://github.com/scott89/STCT" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】</a></p></li></ul></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">CNN-SVM</strong>【ICML’15】<a href="http://120.52.73.80/arxiv.org/pdf/1502.06796.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Online Tracking by Learning Discriminative Saliency Map with Convolutional Neural Network】</a><a href="http://cvlab.postech.ac.kr/research/CNN_SVM/" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】</a></p>

<ul style="margin-top: 0; padding-left: 2em; margin-bottom: .55em; line-height: 1.6;">
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">MDNet</strong>【CVPR’16】<a href="http://arxiv.org/pdf/1510.07945v2.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Learning Multi-Domain Convolutional Neural Networks for Visual Tracking】</a><a href="https://github.com/HyeonseobNam/MDNet" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】</a><a href="http://cvlab.postech.ac.kr/research/mdnet/" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】</a><a href="http://votchallenge.net/vot2015/download/presentation_Hyeonseob.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【VOT Presentation】</a></p>

<ul style="margin-top: 0; padding-left: 2em; margin-bottom: .55em; line-height: 1.6;">
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">RT-MDNet</strong>【ECCV’18】<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Ilchae_Jung_Real-Time_MDNet_ECCV_2018_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Real-Time MDNet】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">DAT</strong>【NIPS’18】<a href="https://arxiv.org/pdf/1810.03851.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Deep Attentive Tracking via Reciprocative Learning】</a><a href="https://github.com/shipubupt/NIPS2018" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】</a><a href="https://ybsong00.github.io/nips18_tracking/index" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">VITAL</strong>【CVPR’18 Spotlight】<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Song_VITAL_VIsual_Tracking_CVPR_2018_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【VITAL: VIsual Tracking via Adversarial Learning】</a><a href="https://github.com/ybsong00/Vital_release" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】</a><a href="https://ybsong00.github.io/cvpr18_tracking/index" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">TCNN</strong>【2016】<a href="http://arxiv.org/pdf/1608.07242v1.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Modeling and Propagating CNNs in a Tree Structure for Visual Tracking】</a><a href="http://www.votchallenge.net/vot2016/download/44_TCNN.zip" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Code】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">ADNet</strong>【CVPR’17 Soptlight】<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Yun_Action-Decision_Networks_for_CVPR_2017_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Action-Decision Networks for Visual Tracking with Deep Reinforcement Learning】</a><a href="http://openaccess.thecvf.com/content_cvpr_2017/supplemental/Yun_Action-Decision_Networks_for_2017_CVPR_supplemental.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Supp】</a><a href="https://sites.google.com/view/cvpr2017-adnet" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">SANet</strong>【CVPRW’17】<a href="https://arxiv.org/pdf/1611.06878.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【SANet: Structure-Aware Network for Visual Tracking】</a><a href="http://www.dabi.temple.edu/~hbling/code/SANet/SANet.html" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">TSN</strong>【ICCV’17】<a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Teng_Robust_Object_Tracking_ICCV_2017_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Robust Object Tracking based on Temporal and Spatial Deep Networks】</a></p></li></ul></li></ul></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">ROLO</strong>【2016】<a href="http://arxiv.org/pdf/1607.05781v1.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Spatially Supervised Recurrent Convolutional Neural Networks for Visual Object Tracking】</a><a href="https://github.com/Guanghan/ROLO/" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】</a><a href="http://guanghan.info/projects/ROLO/" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">SINT</strong>【CVPR’16】<a href="https://arxiv.org/pdf/1605.05863.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Siamese Instance Search for Tracking】</a><a href="https://staff.fnwi.uva.nl/r.tao/projects/SINT/SINT_proj.html" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】</a></p>

<ul style="margin-top: 0; padding-left: 2em; margin-bottom: .55em; line-height: 1.6;">
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">SINT++</strong>【CVPR’18】<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_SINT_Robust_Visual_CVPR_2018_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【SINT++: Robust Visual Tracking via Adversarial Positive Instance Generation】</a></p></li></ul></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">YCNN</strong>【2016】<a href="https://arxiv.org/pdf/1604.07507v1.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Once for All: a Two-flow Convolutional Neural Network for Visual Tracking】</a></p>

<ul style="margin-top: 0; padding-left: 2em; margin-bottom: .55em; line-height: 1.6;">
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">CRT</strong>【2016】<a href="https://arxiv.org/pdf/1611.04215.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Convolutional Regression for Visual Tracking】</a><a href="https://github.com/chkap/crt" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】</a></p>

<ul style="margin-top: 0; padding-left: 2em; margin-bottom: .55em; line-height: 1.6;">
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">CREST</strong>【ICCV’17 Spotlight】<a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Song_CREST_Convolutional_Residual_ICCV_2017_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【CREST: Convolutional Residual Learning for Visual Tracking】</a><a href="https://github.com/ybsong00/CREST-Release" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】</a><a href="http://www.cs.cityu.edu.hk/~yibisong/iccv17/index.html" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">UCT</strong>【ICCVW’17】<a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w28/Zhu_UCT_Learning_Unified_ICCV_2017_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【UCT: Learning Unified Convolutional Networks for Real-Time Visual Tracking】</a></p></li></ul></li></ul></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">GOTURN</strong>【ECCV’16】<a href="http://davheld.github.io/GOTURN/GOTURN.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Learning to Track at 100 FPS with Deep Regression Networks】</a><a href="https://github.com/davheld/GOTURN" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】</a><a href="http://davheld.github.io/GOTURN/GOTURN.html" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】</a></p>

<ul style="margin-top: 0; padding-left: 2em; margin-bottom: .55em; line-height: 1.6;">
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">DeepLK</strong>【ICRA’18】<a href="http://ci2cv.net/media/papers/deepLK-icra.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Deep-LK for Efficient Adaptive Object Tracking】</a></p></li></ul></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">Learnet</strong>【NIPS’16】<a href="https://arxiv.org/pdf/1606.05233v1.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Learning feed-forward one-shot learners】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">DRT</strong>【TIP’17】<a href="http://ieeexplore.ieee.org/abstract/document/7828108/" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Deep Relative Tracking】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">Re3</strong>【2017】<a href="https://arxiv.org/pdf/1705.06368.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Re3 : Real-Time Recurrent Regression Networks for Object Tracking】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">SiameseFC</strong>【ECCV’16】<a href="https://arxiv.org/pdf/1606.09549.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Fully-Convolutional Siamese Networks for Object Tracking】</a><a href="https://github.com/bertinetto/siamese-fc" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】</a><a href="http://www.robots.ox.ac.uk/~luca/siamese-fc.html" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】</a></p>

<ul style="margin-top: 0; padding-left: 2em; margin-bottom: .55em; line-height: 1.6;">
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">SiamFC-tri</strong>【ECCV’18】<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Xingping_Dong_Triplet_Loss_with_ECCV_2018_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Triplet Loss in Siamese Network for Object Tracking】</a><a href="https://github.com/shenjianbing/TripletTracking" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">MemTrack</strong>【ECCV’18】<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Learning Dynamic Memory Networks for Object Tracking】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">StructSiam</strong>【ECCV’18】<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Yunhua_Zhang_Structured_Siamese_Network_ECCV_2018_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Structured Siamese Network for Real-Time Visual Tracking】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">HART</strong>【NIPS’17】<a href="https://papers.nips.cc/paper/6898-hierarchical-attentive-recurrent-tracking.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Hierarchical Attentive Recurrent Tracking】</a><a href="https://github.com/akosiorek/hart" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">RASNet</strong>【CVPR’18】<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Learning_Attentions_Residual_CVPR_2018_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Learning Attentions: Residual Attentional Siamese Network for High Performance Online Visual Tracking】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">RFL</strong>【ICCVW’17】<a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w28/Yang_Recurrent_Filter_Learning_ICCV_2017_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Recurrent Filter Learning for Visual Tracking】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">Dsiam</strong>【ICCV’17】<a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Guo_Learning_Dynamic_Siamese_ICCV_2017_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Learning Dynamic Siamese Network for Visual Object Tracking】</a><a href="https://github.com/tsingqguo/DSiam" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">EAST</strong>【ICCV’17】<a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Huang_Learning_Policies_for_ICCV_2017_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Learning Policies for Adaptive Tracking with Deep Feature Cascades】</a><a href="http://openaccess.thecvf.com/content_ICCV_2017/supplemental/Huang_Learning_Policies_for_ICCV_2017_supplemental.zip" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Supp】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">SA-Siam</strong>【CVPR’18】<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/He_A_Twofold_Siamese_CVPR_2018_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【A Twofold Siamese Network for Real-Time Object Tracking】</a><a href="https://github.com/Microsoft/SA-Siam" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】</a></p>

<ul style="margin-top: 0; padding-left: 2em; margin-bottom: .55em; line-height: 1.6;">
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">SiamBM</strong>【2018】<a href="http://staff.ustc.edu.cn/~xinmei/publications_pdf/2018/Towards%20a%20Better%20Match%20in%20Siamese%20Network%20Based%20Visual%20Object%20Tracker.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Towards a Better Match in Siamese Network Based Visual Object Tracker】</a></p></li></ul></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">CFNet</strong>【CVPR’17】<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Valmadre_End-To-End_Representation_Learning_CVPR_2017_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【End-to-end representation learning for Correlation Filter based tracking】</a><a href="http://openaccess.thecvf.com/content_cvpr_2017/supplemental/Valmadre_End-To-End_Representation_Learning_2017_CVPR_supplemental.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Supp】</a><a href="https://github.com/bertinetto/cfnet" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】</a><a href="http://www.robots.ox.ac.uk/~luca/cfnet.html" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】</a></p>

<ul style="margin-top: 0; padding-left: 2em; margin-bottom: .55em; line-height: 1.6;">
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">RTINet</strong>【ECCV’18】<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Yingjie_Yao_Joint_Representation_and_ECCV_2018_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Joint Representation and Truncated Inference Learning for Correlation Filter based Tracking】</a></p></li></ul></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">DCFNet</strong>【2017】<a href="https://arxiv.org/pdf/1704.04057.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【DCFNet: Discriminant Correlation Filters Network for Visual Tracking】</a><a href="https://github.com/foolwood/DCFNet#dcfnet-discriminant-correlation-filters-network-for-visual-tracking" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】</a></p>

<ul style="margin-top: 0; padding-left: 2em; margin-bottom: .55em; line-height: 1.6;">
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">SACF</strong>【ECCV’18】<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/mengdan_zhang_Visual_Tracking_via_ECCV_2018_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Visual Tracking via Spatially Aligned Correlation Filters Network】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">FlowTrack</strong>【CVPR’18】<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhu_End-to-End_Flow_Correlation_CVPR_2018_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【End-to-end Flow Correlation Tracking with Spatial-temporal Attention】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">DSNet</strong>【2018】<a href="https://arxiv.org/abs/1812.07049" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【DSNet for Real-Time Driving Scene Semantic Segmentation】</a><a href="https://github.com/s7ev3n/DSNet" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】</a></p></li></ul></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">SiamRPN</strong>【CVPR’18 Spotlight】<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_High_Performance_Visual_CVPR_2018_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【High Performance Visual Tracking with Siamese Region Proposal Network】</a><a href="https://github.com/arbitularov/SiamRPN-PyTorch" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】</a></p>

<ul style="margin-top: 0; padding-left: 2em; margin-bottom: .55em; line-height: 1.6;">
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">DaSiamRPN</strong>【ECCV’18】<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Distractor-aware Siamese Networks for Visual Object Tracking】</a><a href="https://github.com/foolwood/DaSiamRPN" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】</a></p>

<ul style="margin-top: 0; padding-left: 2em; margin-bottom: .55em; line-height: 1.6;">
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">SiamMask</strong>【CVPR’19】<a href="https://arxiv.org/pdf/1812.05050.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Fast Online Object Tracking and Segmentation: A Unifying Approach】</a><a href="https://github.com/foolwood/SiamMask" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【GitHub】</a><a href="http://www.robots.ox.ac.uk/~qwang/SiamMask/" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">SiamRPN++</strong>【CVPR’19 Oral】<a href="https://arxiv.org/pdf/1812.11703.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【SiamRPN++: Evolution of Siamese Visual Tracking with Very Deep Networks】</a><a href="http://bo-li.info/SiamRPN++/" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Project】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">Siamese Cascade-RPN</strong>【CVPR’19】<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Fan_Siamese_Cascaded_Region_Proposal_Networks_for_Real-Time_Visual_Tracking_CVPR_2019_paper.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【Siamese Cascaded Region Proposal Networks for Real-Time Visual Tracking】</a></p></li>
<li style="line-height: 1.6;"><p style="margin: 0; line-height: 1.6;"><strong style="font-weight: bold; line-height: 1.6;">SPM</strong>【CVPR’19】<a href="https://arxiv.org/pdf/1904.04452.pdf" style="background: transparent; color: #1980e6; text-decoration: none;" target="_blank">【SPM-Tracker: Series-Parallel Matching for Real-Time Visual Object Tracking】</a></p></li></ul></li></ul></li></ul></li>
</ul></div><center style="display:none !important;visibility:collapse !important;height:0 !important;white-space:nowrap;width:100%;overflow:hidden">%23%20Visual%20Tracker%20Roadmap%0A%0A@%28CV%20Learning%29%5BCV%2C%20Summary%2C%20Video%2C%20Tracking%5D%0A%0A%3E**%u4F5C%u8005%uFF1A%5Bhuuuuusy%5D%28https%3A//github.com/huuuuusy%29**%0A%3E**%u4FEE%u6539%u65E5%u671F%uFF1A2019-09-05**%0A%0A%21%5Bimage%5D%28https%3A//user-images.githubusercontent.com/32117906/62909573-b380cd00-bdaf-11e9-8ce0-495777184f02.png%29%0A%u56FE%u6E90%uFF1A%20%5B%u738B%u5F3A%u535A%u58EB%5D%28https%3A//github.com/foolwood%29%0A%0A%u672C%u6587%u57FA%u4E8E%u738B%u5F3A%u535A%u58EB%u7684roadmap%u5E76%u6DFB%u52A0%u76F8%u5173%u8BBA%u6587%u7684%u94FE%u63A5%u3002%0A%0A%23%201%20Benchmark%0A%0A%23%23%201.1%20%u7ECF%u5178%0A%0A-%20**OBT**%0A%20%20%20%20-%20**OTB100**%u3010TPAMI%2715%u3011%20%5B%u3010Object%20tracking%20benchmark%u3011%20%5D%28https%3A//ieeexplore.ieee.org/document/7001050%29%0A%20%20%20%20-%20**OTB50**%u3010CVPR%2713%u3011%5B%u3010Online%20object%20tracking%3A%20A%20benchmark%u3011%5D%28http%3A//faculty.ucmerced.edu/mhyang/papers/cvpr13_benchmark.pdf%29%0A%0A-%20%5B**VOT**%5D%28http%3A//votchallenge.net/%29%0A%20%20%20%20-%20%5B**VOT2019%20Challenge**%5D%28http%3A//votchallenge.net/vot2019/%29%0A%20%20%20%20-%20%5B**VOT2018**%5D%28http%3A//votchallenge.net/vot2018/%29%u3010ECCV%2718%u3011%5B%u3010The%20sixth%20Visual%20Object%20Tracking%20VOT2018%20challenge%20results%u3011%5D%28https%3A//prints.vicos.si/publications/365/the-sixth-visual-object-tracking-vot2018-challenge-results%29%0A%20%20%20%20-%20%5B**VOT2017**%5D%28http%3A//votchallenge.net/vot2017/%29%u3010ICCV%2717%u3011%20%5B%u3010The%20Visual%20Object%20Tracking%20VOT2017%20Challenge%20Results%u3011%5D%28https%3A//prints.vicos.si/publications/359/the-visual-object-tracking-vot2017-challenge-results%29%0A%20%20%20%20-%20%5B**VOT2016**%5D%28http%3A//votchallenge.net/vot2016/%29%u3010ECCV%2716%u3011%5B%u3010The%20Visual%20Object%20Tracking%20VOT2016%20challenge%20results%u3011%5D%28https%3A//prints.vicos.si/publications/340/the-visual-object-tracking-vot2016-challenge-results%29%0A%20%20%20%20-%20%5B**VOT2015**%5D%28http%3A//votchallenge.net/vot2015/%29%u3010ICCV%2715%u3011%5B%u3010The%20Visual%20Object%20Tracking%20VOT2015%20challenge%20results%u3011%5D%28https%3A//prints.vicos.si/publications/325/the-visual-object-tracking-vot2015-challenge-results%29%0A%20%20%20%20-%20%5B**VOT2014**%5D%28http%3A//votchallenge.net/vot2014/%29%u3010ECCV%2714%u3011%5B%u3010The%20Visual%20Object%20Tracking%20VOT2014%20challenge%20results%u3011%5D%28https%3A//prints.vicos.si/publications/315/the-visual-object-tracking-vot2014-challenge-results%29%0A%20%20%20%20-%20%5B**VOT2013**%5D%28http%3A//votchallenge.net/vot2013/%29%u3010ICCV%2713%u3011%5B%u3010The%20Visual%20Object%20Tracking%20VOT2013%20challenge%20results%u3011%5D%28https%3A//prints.vicos.si/publications/304/the-visual-object-tracking-vot2013-challenge-results%29%0A%0A%23%23%201.2%20%u5176%u4ED6%0A%0A-%20**GOT-10K**%u30102018%u3011%20%5B%u3010GOT-10k%3A%20A%20Large%20High-Diversity%20Benchmark%20for%20Generic%20Object%20Tracking%20in%20the%20Wild%u3011%5D%28https%3A//arxiv.org/abs/1810.11981%29%5B%u3010GitHub%u3011%5D%28https%3A//github.com/got-10k/toolkit%29%5B%u3010Project%u3011%5D%28http%3A//got-10k.aitestunion.com/%29%0A-%20**LaSOT**%u3010CVPR%2719%u3011%5B%u3010LaSOT%3A%20A%20High-quality%20Benchmark%20for%20Large-scale%20Single%20Object%20Tracking%u3011%5D%28http%3A//openaccess.thecvf.com/content_CVPR_2019/papers/Fan_LaSOT_A_High-Quality_Benchmark_for_Large-Scale_Single_Object_Tracking_CVPR_2019_paper.pdf%29%5B%u3010Project%u3011%5D%28https%3A//cis.temple.edu/lasot/%29%0A-%20**OxUvA**%u3010ECCV%2718%u3011%5B%u3010Long-term%20Tracking%20in%20the%20Wild%3A%20A%20Benchmark%u3011%5D%28http%3A//openaccess.thecvf.com/content_ECCV_2018/papers/Efstratios_Gavves_Long-term_Tracking_in_ECCV_2018_paper.pdf%29%5B%u3010Project%u3011%5D%28https%3A//oxuva.github.io/long-term-tracking-benchmark/%29%5B%u3010GitHub%u3011%5D%28https%3A//github.com/oxuva/long-term-tracking-benchmark%23oxuva-long-term-tracking-benchmark-eccv18%29%0A-%20**TrackingNet**%u3010ECCV%2718%u3011%5B%u3010TrackingNet%3A%20A%20Large-Scale%20Dataset%20and%20Benchmark%20for%20Object%20Tracking%20in%20the%20Wild%u3011%5D%28http%3A//openaccess.thecvf.com/content_ECCV_2018/papers/Matthias_Muller_TrackingNet_A_Large-Scale_ECCV_2018_paper.pdf%29%20%5B%u3010Project%u3011%5D%28https%3A//tracking-net.org/%29%20%5B%u3010GitHub%u3011%5D%28https%3A//github.com/SilvioGiancola/TrackingNet-devkit%29%0A-%20**UAVDT**%u3010ECCV%2718%u3011%5B%u3010The%20Unmanned%20Aerial%20Vehicle%20Benchmark%3A%20Object%20Detection%20and%20Tracking%u3011%5D%28https%3A//arxiv.org/pdf/1804.00518.pdf%29%5B%u3010Project%u3011%5D%28https%3A//sites.google.com/site/daviddo0323/projects/uavdt%29%0A-%20**AMP**%u3010ICCV%2717%u3011%5B%u3010Beyond%20Standard%20Benchmarks%3A%20Parameterizing%20Performance%20Evaluation%20in%20Visual%20Object%20Tracking%u3011%5D%28http%3A//openaccess.thecvf.com/content_ICCV_2017/papers/Zajc_Beyond_Standard_Benchmarks_ICCV_2017_paper.pdf%29%0A-%20**Nfs**%u3010ICCV%2717%u3011%5B%u3010Need%20for%20Speed%3A%20A%20Benchmark%20for%20Higher%20Frame%20Rate%20Object%20Tracking%u3011%5D%28http%3A//openaccess.thecvf.com/content_ICCV_2017/papers/Galoogahi_Need_for_Speed_ICCV_2017_paper.pdf%29%5B%u3010Project%u3011%5D%28http%3A//ci2cv.net/nfs/index.html%29%0A-%20**DTB70**%u3010AAAI%2717%u3011%5B%u3010Visual%20Object%20Tracking%20for%20Unmanned%20Aerial%20Vehicles%3A%20A%20Benchmark%20and%20New%20Motion%20Models%u3011%5D%28http%3A//aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14338/14292%29%5B%u3010GitHub%u3011%5D%28https%3A//github.com/flyers/drone-tracking%29%0A-%20**UAV123**%u3010ECCV%2716%u3011%5B%u3010A%20Benchmark%20and%20Simulator%20for%20UAV%20Tracking%u3011%5D%28https%3A//ivul.kaust.edu.sa/Documents/Publications/2016/A%2520Benchmark%2520and%2520Simulator%2520for%2520UAV%2520Tracking.pdf%29%20%5B%u3010Project%u3011%5D%28https%3A//ivul.kaust.edu.sa/Pages/pub-benchmark-simulator-uav.aspx%29%0A-%20**TColor-128**%u3010TIP%2715%u3011%5B%u3010Encoding%20color%20information%20for%20visual%20tracking%3A%20Algorithms%20and%20benchmark%u3011%5D%28http%3A//www.dabi.temple.edu/%7Ehbling/publication/TColor-128.pdf%29%5B%u3010Project%u3011%5D%28http%3A//www.dabi.temple.edu/%7Ehbling/data/TColor-128/TColor-128.html%29%0A-%20**NUS-PRO**%u3010PAMI%2715%u3011%20%5B%u3010NUS-PRO%3A%20A%20New%20Visual%20Tracking%20Challenge%u3011%5D%28http%3A//faculty.ucmerced.edu/mhyang/papers/pami15_nus_pro.pdf%29%5B%u3010Project%u3011%5D%28https%3A//sites.google.com/site/li00annan/nus-pro%29%0A-%20**ALOV300+**%u3010PAMI%2714%u3011%5B%u3010Visual%20Tracking%3A%20An%20Experimental%20Survey%u3011%5D%28http%3A//crcv.ucf.edu/papers/Tracking_Survey.pdf%29%5B%u3010Project%u3011%5D%28http%3A//imagelab.ing.unimore.it/dsm/%29%0A-%20**PTB**%u3010ICCV%2713%u3011%5B%u3010Tracking%20Revisited%20using%20RGBD%20Camera%3A%20Unified%20Benchmark%20and%20Baselines%u3011%5D%28http%3A//vision.princeton.edu/projects/2013/tracking/paper.pdf%29%20%5B%u3010Project%u3011%5D%28http%3A//tracking.cs.princeton.edu/%29%0A%0A%23%23%202%20%u6570%u636E%u5E93%0A%0A%23%23%23%202.1%20%5BDAVIS%5D%28https%3A//davischallenge.org/%29%0A-%20Datasets%0A%20%20%20%20-%20%5BDAVIS2017%5D%28https%3A//davischallenge.org/davis2017/code.html%29%0A%20%20%20%20-%20%5BDAVIS2016%5D%28https%3A//davischallenge.org/davis2016/code.html%29%0A-%20Publications%0A%20%20%20%20-%20%5BThe%202019%20DAVIS%20Challenge%20on%20VOS%3A%20Unsupervised%20Multi-Object%20Segmentation%5D%28https%3A//arxiv.org/pdf/1905.00737%29%0A%20%20%20%20-%20%5BThe%202018%20DAVIS%20Challenge%20on%20Video%20Object%20Segmentation%5D%28https%3A//arxiv.org/abs/1803.00557%29%0A%20%20%20%20-%20%5BThe%202017%20DAVIS%20Challenge%20on%20Video%20Object%20Segmentation%5D%28https%3A//arxiv.org/abs/1704.00675%29%0A%20%20%20%20-%20%5B%u3010CVPR%2716%u3011A%20Benchmark%20Dataset%20and%20Evaluation%20Methodology%20for%20Video%20Object%20Segmentation%5D%28https%3A//www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Perazzi_A_Benchmark_Dataset_CVPR_2016_paper.pdf%29%0A%0A%23%23%23%202.2%20%5BILSVRC%5D%28http%3A//image-net.org/challenges/LSVRC%29%0A-%20%5BILSVRC2017-VID%5D%28http%3A//image-net.org/challenges/LSVRC/2017/%29%0A-%20%5BILSVRC2016-VID%5D%28http%3A//image-net.org/challenges/LSVRC/2016/%29%0A-%20%5BILSVRC2015-VID%5D%28http%3A//image-net.org/challenges/LSVRC/2015/%29%0A%0A%23%23%23%202.3%20YouTube%20BoundingBox%0A-%20%5B%u3010CVPR%2717%u3011YouTube-BoundingBoxes%3A%20A%20Large%20High-Precision%20Human-Annotated%20Data%20Set%20for%20Object%20Detection%20in%20Video%5D%28http%3A//openaccess.thecvf.com/content_cvpr_2017/papers/Real_YouTube-BoundingBoxes_A_Large_CVPR_2017_paper.pdf%29%0A%20%20%20%20-%20%5BGitHub%5D%28https%3A//github.com/mbuckler/youtube-bb%29%0A%0A%23%23%203%20%u76F8%u5173%u6EE4%u6CE2%0A%0A**MOSSE**%u3010CVPR%2710%u3011%5B%u3010Visual%20object%20tracking%20using%20adaptive%20correlation%20filters%u3011%5D%28http%3A//www.cs.colostate.edu/%7Edraper/papers/bolme_cvpr10.pdf%29%0A-%20**MCCF**%u3010ICCV%2713%u3011%5B%u3010Multi-Channel%20Correlation%20Filters%u3011%5D%28https%3A//www.cv-foundation.org/openaccess/content_iccv_2013/papers/Galoogahi_Multi-channel_Correlation_Filters_2013_ICCV_paper.pdf%29%5B%u3010Project%u3011%5D%28http%3A//www.hamedkiani.com/mccf.html%29%0A%20%20%20%20-%20**CFLB**%u3010ICCV%2715%u3011%5B%u3010Correlation%20Filters%20with%20Limited%20Boundaries%u3011%5D%28https%3A//www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Galoogahi_Correlation_Filters_With_2015_CVPR_paper.pdf%29%5B%u3010Project%u3011%5D%28http%3A//www.hamedkiani.com/cfwlb.html%29%0A%20%20%20%20%20%20%20%20-%20**BACF**%u3010ICCV%2717%u3011%5B%u3010Learning%20Background-Aware%20Correlation%20Filters%20for%20Visual%20Tracking%u3011%5D%28http%3A//openaccess.thecvf.com/content_ICCV_2017/papers/Galoogahi_Learning_Background-Aware_Correlation_ICCV_2017_paper.pdf%29%5B%u3010Project%u3011%5D%28http%3A//www.hamedkiani.com/bacf.html%29%0A-%20**CSK**%u3010ECCV%2712%u3011%5B%u3010Exploiting%20the%20circulant%20structure%20of%20tracking-by-detection%20with%20kernels%u3011%5D%28http%3A//www.robots.ox.ac.uk/%7Ejoao/publications/henriques_eccv2012.pdf%29%0A%20%20%20%20-%20**CN**%20%u3010CVPR%2714%u3011%5B%u3010Adaptive%20color%20attributes%20for%20real-time%20visual%20tracking%u3011%5D%28https%3A//www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Danelljan_Adaptive_Color_Attributes_2014_CVPR_paper.pdf%29%5B%u3010Project%u3011%5D%28http%3A//www.cvl.isy.liu.se/research/objrec/visualtracking/colvistrack/index.html%29%0A%20%20%20%20-%20**STC**%u3010ECCV%2714%u3011%5B%u3010Fast%20Tracking%20via%20Spatio-Temporal%20Context%20Learning%u3011%5D%28https%3A//arxiv.org/pdf/1311.1939v1.pdf%29%0A%20%20%20%20-%20**SAMF**%u3010ECCVW%2714%u3011%5B%u3010A%20Scale%20Adaptive%20Kernel%20Correlation%20Filter%20Tracker%20with%20Feature%20Integration%u3011%5D%28http%3A//github.com/ihpdep/ihpdep.github.io/raw/master/papers/eccvw14_samf.pdf%29%5B%u3010GitHub%u3011%5D%28https%3A//github.com/ihpdep/samf%29%0A%20%20%20%20-%20**DSST**%u3010BMVC%2714%u3011%20%5B%u3010Accurate%20Scale%20Estimation%20for%20Robust%20Visual%20Tracking%u3011%5D%28http%3A//www.bmva.org/bmvc/2014/files/paper038.pdf%29%u3010TPAMI%2717%u3011%20%5B%u3010Paper%u3011%5D%28https%3A//arxiv.org/abs/1609.06141v1%29%5B%u3010Project%u3011%5D%28http%3A//www.cvl.isy.liu.se/research/objrec/visualtracking/scalvistrack/index.html%29%0A%20%20%20%20-%20**KCF**%20%u3010TPAMI%2715%u3011%5B%u3010High-Speed%20Tracking%20with%20Kernelized%20Correlation%20Filters%u3011%5D%28https%3A//arxiv.org/abs/1404.7584%29%20%5B%u3010GitHub%u3011%5D%28https%3A//github.com/foolwood/KCF%29%0A%20%20%20%20%20%20%20%20-%20**CACF**%u3010CVPR%2717%20Oral%u3011%5B%u3010Context-Aware%20Correlation%20Filter%20Tracking%u3011%5D%28http%3A//openaccess.thecvf.com/content_cvpr_2017/papers/Mueller_Context-Aware_Correlation_Filter_CVPR_2017_paper.pdf%29%5B%u3010GitHub%u3011%5D%28https%3A//github.com/thias15/Context-Aware-CF-Tracking%29%0A%20%20%20%20%20%20%20%20-%20**Staple**%u3010CVPR%2716%u3011%5B%u3010Staple%3A%20Complementary%20Learners%20for%20Real-Time%20Tracking%u3011%5D%28https%3A//www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Bertinetto_Staple_Complementary_Learners_CVPR_2016_paper.pdf%29%20%5B%u3010Project%u3011%5D%28http%3A//www.robots.ox.ac.uk/%7Eluca/staple.html%29%5B%u3010GitHub%u3011%5D%28https%3A//github.com/bertinetto/staple%29%0A%20%20%20%20%20%20%20%20-%20**RAJSSC**%u3010ICCVW%2715%u3011%5B%u3010Joint%20Scale-Spatial%20Correlation%20Tracking%20with%20Adaptive%20Rotation%20Estimation%u3011%5D%28http%3A//www.cv-foundation.org//openaccess/content_iccv_2015_workshops/w14/papers/Zhang_Joint_Scale-Spatial_Correlation_ICCV_2015_paper.pdf%29%5B%u3010Poster%u3011%5D%28http%3A//www.votchallenge.net/vot2015/download/poster_Mengdan_Zhang.pdf%29%0A%20%20%20%20%20%20%20%20-%20**KCF_MTS**%u3010ICCVW%2715%u3011%5B%u3010Multi-Template%20Scale-Adaptive%20Kernelized%20Correlation%20Filters%u3011%5D%28https%3A//www.cv-foundation.org/openaccess/content_iccv_2015_workshops/w14/papers/Bibi_Multi-Template_Scale-Adaptive_Kernelized_ICCV_2015_paper.pdf%29%5B%u3010GitHub%u3011%5D%28https%3A//github.com/adelbibi/Multi-Template-Scale-Adaptive-Kernelized-Correlation-Filters%29%0A%20%20%20%20%20%20%20%20-%20**LCT**%u3010CVPR%2715%u3011%5B%u3010Long-term%20Correlation%20Tracking%u3011%5D%28https%3A//www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ma_Long-Term_Correlation_Tracking_2015_CVPR_paper.pdf%29%u3010IJCV%2718%u3011%5B%u3010Paper%u3011%5D%28http%3A//faculty.ucmerced.edu/mhyang/papers/ijcv18_lct.pdf%29%5B%u3010GitHub%u3011%5D%28https%3A//github.com/chaoma99/lct-tracker%29%0A%20%20%20%20%20%20%20%20-%20**MUSTer**%u3010CVPR%2715%u3011%5B%u3010MUlti-Store%20Tracker%20%28MUSTer%29%3A%20A%20Cognitive%20Psychology%20Inspired%20Approach%20to%20Object%20Tracking%u3011%5D%28https%3A//www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Hong_MUlti-Store_Tracker_MUSTer_2015_CVPR_paper.pdf%29%5B%u3010Project%u3011%5D%28https%3A//sites.google.com/site/zhibinhong4131/Projects/muster%29%0A%20%20%20%20%20%20%20%20-%20**CF2**%u3010ICCV%2715%u3011%5B%u3010Hierarchical%20Convolutional%20Features%20for%20Visual%20Tracking%u3011%5D%28http%3A//faculty.ucmerced.edu/mhyang/papers/iccv15_tracking.pdf%29%5B%u3010Project%u3011%20%5D%28https%3A//sites.google.com/site/jbhuang0604/publications/cf2%29%20%5B%u3010GitHub%u3011%5D%28https%3A//github.com/jbhuang0604/CF2%29%0A%20%20%20%20%20%20%20%20%20%20%20%20-%20**HDT**%u3010CVPR%2716%u3011%5B%u3010Hedged%20Deep%20Tracking%u3011%5D%28http%3A//faculty.ucmerced.edu/mhyang/papers/cvpr16_hedge_tracking.pdf%29%5B%u3010Project%u3011%5D%28https%3A//sites.google.com/site/yuankiqi/hdt/%29%0A%20%20%20%20%20%20%20%20%20%20%20%20-%20**MCPF**%u3010TPAMI%2717%u3011%5B%u3010Learning%20Multi-task%20Correlation%20Particle%20Filters%20for%20Visual%20Tracking%u3011%5D%28https%3A//ieeexplore.ieee.org/document/8267285/%29%5B%u3010Project%u3011%20%5D%28http%3A//nlpr-web.ia.ac.cn/mmc/homepage/tzzhang/lmcpf.html%29%5B%u3010Code%u3011%5D%28http%3A//nlpr-web.ia.ac.cn/mmc/homepage/tzzhang/Project_Tianzhu/zhang_mcpf/Source_Code/Source_Code.zip%29%0A%20%20%20%20%20%20%20%20%20%20%20%20-%20**IBCCF**%u3010ICCVW%2717%u3011%5B%u3010Integrating%20Boundary%20and%20Center%20Correlation%20Filters%20for%20Visual%20Tracking%20With%20Aspect%20Ratio%20Variation%u3011%5D%28http%3A//openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w28/Li_Integrating_Boundary_and_ICCV_2017_paper.pdf%29%5B%u3010GitHub%u3011%5D%28https%3A//github.com/lifeng9472/IBCCF%29%0A%20%20%20%20%20%20%20%20-%20**SRDCF**%u3010ICCV%2715%u3011%5B%u3010Learning%20Spatially%20Regularized%20Correlation%20Filters%20for%20Visual%20Tracking%u3011%5D%28https%3A//www.cv-foundation.org/openaccess/content_iccv_2015/papers/Danelljan_Learning_Spatially_Regularized_ICCV_2015_paper.pdf%29%5B%u3010Project%u3011%20%5D%28http%3A//www.cvl.isy.liu.se/research/objrec/visualtracking/regvistrack/index.html%29%0A%20%20%20%20%20%20%20%20%20%20%20%20-%20**STRCF**%20%u3010CVPR%2718%u3011%5B%u3010Learning%20Spatial-Temporal%20Regularized%20Correlation%20Filters%20for%20Visual%20Tracking%u3011%5D%28http%3A//openaccess.thecvf.com/content_cvpr_2018/papers/Li_Learning_Spatial-Temporal_Regularized_CVPR_2018_paper.pdf%20%29%5B%u3010GitHub%u3011%20%5D%28https%3A//github.com/lifeng9472/STRCF%29%0A%20%20%20%20%20%20%20%20%20%20%20%20-%20**CSR-DCF**%u3010CVPR%2717%u3011%5B%u3010Discriminative%20Correlation%20Filter%20with%20Channel%20and%20Spatial%20Reliability%u3011%5D%28http%3A//openaccess.thecvf.com/content_cvpr_2017/papers/Lukezic_Discriminative_Correlation_Filter_CVPR_2017_paper.pdf%29%5B%u3010Supp%u3011%5D%28http%3A//openaccess.thecvf.com/content_cvpr_2017/supplemental/Lukezic_Discriminative_Correlation_Filter_2017_CVPR_supplemental.pdf%29%20%5B%u3010GitHub%u3011%20%5D%28https%3A//github.com/alanlukezic/csr-dcf%29%0A%20%20%20%20%20%20%20%20%20%20%20%20-%20**SRDCFdecon**%20%u3010CVPR%2716%u3011%5B%u3010Adaptive%20Decontamination%20of%20the%20Training%20Set%3A%20A%20Unified%20Formulation%20for%20Discriminative%20Visual%20Tracking%u3011%5D%28https%3A//www.cvl.isy.liu.se/research/objrec/visualtracking/decontrack/AdaptiveDecon_CVPR16.pdf%29%20%5B%u3010Project%u3011%20%5D%28https%3A//www.cvl.isy.liu.se/research/objrec/visualtracking/decontrack/index.html%29%0A%20%20%20%20%20%20%20%20%20%20%20%20-%20**DeepSRDCF**%20%20%u3010ICCV%2715%u3011%5B%u3010Convolutional%20Features%20for%20Correlation%20Filter%20Based%20Visual%20Tracking%u3011%5D%28https%3A//www.cvl.isy.liu.se/research/objrec/visualtracking/regvistrack/ConvDCF_ICCV15_VOTworkshop.pdf%29%20%5B%u3010Project%u3011%20%5D%28https%3A//www.cvl.isy.liu.se/research/objrec/visualtracking/regvistrack/%29%0A%20%20%20%20%20%20%20%20%20%20%20%20-%20**DMSRDCF**%u3010ICPR%2716%20Best%20Paper%u3011%5B%u3010Deep%20Motion%20Features%20for%20Visual%20Tracking%u3011%5D%28https%3A//arxiv.org/pdf/1612.06615v1.pdf%29%20%0A%20%20%20%20%20%20%20%20%20%20%20%20-%20**C-COT**%u3010ECCV%2716%u3011%5B%u3010Beyond%20Correlation%20Filters%3A%20Learning%20Continuous%20Convolution%20Operators%20for%20Visual%20Trackin%u3011%5D%28http%3A//www.cvl.isy.liu.se/research/objrec/visualtracking/conttrack/C-COT_ECCV16.pdf%29%20%5B%u3010Project%u3011%20%5D%28http%3A//www.cvl.isy.liu.se/research/objrec/visualtracking/conttrack/index.html%29%5B%u3010GitHub%u3011%20%5D%28https%3A//github.com/martin-danelljan/Continuous-ConvOp%29%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20-%20**CFCF**%u3010TIP%2718%u3011%5B%u3010Good%20Features%20to%20Correlate%20for%20Visual%20Tracking%u3011%5D%28https%3A//www.researchgate.net/publication/316428659_Good_Features_to_Correlate_for_Visual_Tracking%29%20%5B%u3010GitHub%u3011%20%5D%28https%3A//github.com/egundogdu/CFCF%29%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20-%20**ECO**%u3010CVPR%2717%u3011%5B%u3010ECO%3A%20Efficient%20Convolution%20Operators%20for%20Tracking%u3011%5D%28http%3A//openaccess.thecvf.com/content_cvpr_2017/papers/Danelljan_ECO_Efficient_Convolution_CVPR_2017_paper.pdf%29%20%5B%u3010Supp%u3011%5D%28http%3A//openaccess.thecvf.com/content_cvpr_2017/supplemental/Danelljan_ECO_Efficient_Convolution_2017_CVPR_supplemental.pdf%29%5B%u3010Project%u3011%20%5D%28http%3A//www.cvl.isy.liu.se/research/objrec/visualtracking/ecotrack/index.html%29%5B%u3010GitHub%u3011%20%5D%28https%3A//github.com/martin-danelljan/ECO%29%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20-%20**UPDT**%u3010ECCV%2718%u3011%5B%u3010Unveiling%20the%20Power%20of%20Deep%20Tracking%u3011%5D%28http%3A//openaccess.thecvf.com/content_ECCV_2018/papers/Goutam_Bhat_Unveiling_the_Power_ECCV_2018_paper.pdf%29%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20-%20**CFWCR**%u3010ICCVW%2717%u3011%5B%u3010Correlation%20Filters%20With%20Weighted%20Convolution%20Responses%u3011%5D%28http%3A//openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w28/He_Correlation_Filters_With_ICCV_2017_paper.pdf%29%20%5B%u3010GitHub%u3011%20%5D%28https%3A//github.com/he010103/CFWCR%29%0A%0A%23%23%204%20%u6DF1%u5EA6%u5B66%u4E60%0A%0A-%20**ATOM**%u3010CVPR%2719%20Oral%u3011%5B%u3010ATOM%3A%20Accurate%20Tracking%20by%20Overlap%20Maximization%u3011%5D%28https%3A//arxiv.org/pdf/1811.07628.pdf%29%5B%u3010GitHub%u3011%5D%28https%3A//github.com/visionml/pytracking%29%0A-%20**DLT**%u3010NIPS%2713%u3011%5B%u3010Learning%20A%20Deep%20Compact%20Image%20Representation%20for%20Visual%20Tracking%u3011%5D%28http%3A//winsty.net/papers/dlt.pdf%29%5B%u3010Project%u3011%5D%28http%3A//winsty.net/dlt.html%29%0A%20%20%20%20-%20**SO-DLT**%u30102015%u3011%5B%u3010Transferring%20Rich%20Feature%20Hierarchies%20for%20Robust%20Visual%20Tracking%u3011%5D%28https%3A//arxiv.org/pdf/1501.04587v2.pdf%29%5B%u3010Code%u3011%5D%28http%3A//www.votchallenge.net/vot2016/download/08_SO-DLT.zip%29%0A-%20**FCNT**%u3010ICCV%2715%u3011%5B%u3010Visual%20Tracking%20with%20Fully%20Convolutional%20Networks%u3011%5D%28http%3A//202.118.75.4/lu/Paper/ICCV2015/iccv15_lijun.pdf%29%5B%u3010GitHub%u3011%5D%28https%3A//github.com/scott89/FCNT%29%5B%u3010Project%u3011%5D%28http%3A//scott89.github.io/FCNT/%29%0A%20%20%20%20-%20**STCT**%u3010CVPR%2716%u3011%5B%u3010STCT%3A%20Sequentially%20Training%20Convolutional%20Networks%20for%20Visual%20Tracking%u3011%5D%28http%3A//www.ee.cuhk.edu.hk/%7Ewlouyang/Papers/WangLJ_CVPR16.pdf%29%5B%u3010GitHub%u3011%5D%28https%3A//github.com/scott89/STCT%29%0A-%20**CNN-SVM**%u3010ICML%2715%u3011%5B%u3010Online%20Tracking%20by%20Learning%20Discriminative%20Saliency%20Map%20with%20Convolutional%20Neural%20Network%u3011%5D%28http%3A//120.52.73.80/arxiv.org/pdf/1502.06796.pdf%29%5B%u3010Project%u3011%5D%28http%3A//cvlab.postech.ac.kr/research/CNN_SVM/%29%0A%20%20%20%20-%20**MDNet**%u3010CVPR%2716%u3011%5B%u3010Learning%20Multi-Domain%20Convolutional%20Neural%20Networks%20for%20Visual%20Tracking%u3011%5D%28http%3A//arxiv.org/pdf/1510.07945v2.pdf%29%5B%u3010GitHub%u3011%5D%28https%3A//github.com/HyeonseobNam/MDNet%29%5B%u3010Project%u3011%5D%28http%3A//cvlab.postech.ac.kr/research/mdnet/%29%5B%u3010VOT%20Presentation%u3011%5D%28http%3A//votchallenge.net/vot2015/download/presentation_Hyeonseob.pdf%29%0A%20%20%20%20%20%20%20%20-%20**RT-MDNet**%u3010ECCV%2718%u3011%5B%u3010Real-Time%20MDNet%u3011%5D%28http%3A//openaccess.thecvf.com/content_ECCV_2018/papers/Ilchae_Jung_Real-Time_MDNet_ECCV_2018_paper.pdf%29%0A%20%20%20%20%20%20%20%20-%20**DAT**%u3010NIPS%2718%u3011%5B%u3010Deep%20Attentive%20Tracking%20via%20Reciprocative%20Learning%u3011%5D%28https%3A//arxiv.org/pdf/1810.03851.pdf%29%5B%u3010GitHub%u3011%5D%28https%3A//github.com/shipubupt/NIPS2018%29%5B%u3010Project%u3011%5D%28https%3A//ybsong00.github.io/nips18_tracking/index%29%0A%20%20%20%20%20%20%20%20-%20**VITAL**%u3010CVPR%2718%20Spotlight%u3011%5B%u3010VITAL%3A%20VIsual%20Tracking%20via%20Adversarial%20Learning%u3011%5D%28http%3A//openaccess.thecvf.com/content_cvpr_2018/papers/Song_VITAL_VIsual_Tracking_CVPR_2018_paper.pdf%29%5B%u3010GitHub%u3011%5D%28https%3A//github.com/ybsong00/Vital_release%29%5B%u3010Project%u3011%5D%28https%3A//ybsong00.github.io/cvpr18_tracking/index%29%0A%20%20%20%20%20%20%20%20-%20**TCNN**%u30102016%u3011%5B%u3010Modeling%20and%20Propagating%20CNNs%20in%20a%20Tree%20Structure%20for%20Visual%20Tracking%u3011%5D%28http%3A//arxiv.org/pdf/1608.07242v1.pdf%29%5B%u3010Code%u3011%5D%28http%3A//www.votchallenge.net/vot2016/download/44_TCNN.zip%29%0A%20%20%20%20%20%20%20%20-%20**ADNet**%u3010CVPR%2717%20Soptlight%u3011%5B%u3010Action-Decision%20Networks%20for%20Visual%20Tracking%20with%20Deep%20Reinforcement%20Learning%u3011%5D%28http%3A//openaccess.thecvf.com/content_cvpr_2017/papers/Yun_Action-Decision_Networks_for_CVPR_2017_paper.pdf%29%5B%u3010Supp%u3011%5D%28http%3A//openaccess.thecvf.com/content_cvpr_2017/supplemental/Yun_Action-Decision_Networks_for_2017_CVPR_supplemental.pdf%29%5B%u3010Project%u3011%5D%28https%3A//sites.google.com/view/cvpr2017-adnet%29%0A%20%20%20%20%20%20%20%20-%20**SANet**%u3010CVPRW%2717%u3011%5B%u3010SANet%3A%20Structure-Aware%20Network%20for%20Visual%20Tracking%u3011%5D%28https%3A//arxiv.org/pdf/1611.06878.pdf%29%5B%u3010Project%u3011%5D%28http%3A//www.dabi.temple.edu/%7Ehbling/code/SANet/SANet.html%29%0A%20%20%20%20%20%20%20%20-%20**TSN**%u3010ICCV%2717%u3011%5B%u3010Robust%20Object%20Tracking%20based%20on%20Temporal%20and%20Spatial%20Deep%20Networks%u3011%5D%28http%3A//openaccess.thecvf.com/content_ICCV_2017/papers/Teng_Robust_Object_Tracking_ICCV_2017_paper.pdf%29%0A-%20**ROLO**%u30102016%u3011%5B%u3010Spatially%20Supervised%20Recurrent%20Convolutional%20Neural%20Networks%20for%20Visual%20Object%20Tracking%u3011%5D%28http%3A//arxiv.org/pdf/1607.05781v1.pdf%29%5B%u3010GitHub%u3011%5D%28https%3A//github.com/Guanghan/ROLO/%29%5B%u3010Project%u3011%5D%28http%3A//guanghan.info/projects/ROLO/%29%0A-%20**SINT**%u3010CVPR%2716%u3011%5B%u3010Siamese%20Instance%20Search%20for%20Tracking%u3011%5D%28https%3A//arxiv.org/pdf/1605.05863.pdf%29%5B%u3010Project%u3011%5D%28https%3A//staff.fnwi.uva.nl/r.tao/projects/SINT/SINT_proj.html%29%0A%20%20%20%20-%20**SINT++**%u3010CVPR%2718%u3011%5B%u3010SINT++%3A%20Robust%20Visual%20Tracking%20via%20Adversarial%20Positive%20Instance%20Generation%u3011%5D%28http%3A//openaccess.thecvf.com/content_cvpr_2018/papers/Wang_SINT_Robust_Visual_CVPR_2018_paper.pdf%29%0A-%20**YCNN**%u30102016%u3011%5B%u3010Once%20for%20All%3A%20a%20Two-flow%20Convolutional%20Neural%20Network%20for%20Visual%20Tracking%u3011%5D%28https%3A//arxiv.org/pdf/1604.07507v1.pdf%29%0A%20%20%20%20-%20**CRT**%u30102016%u3011%5B%u3010Convolutional%20Regression%20for%20Visual%20Tracking%u3011%5D%28https%3A//arxiv.org/pdf/1611.04215.pdf%29%5B%u3010GitHub%u3011%5D%28https%3A//github.com/chkap/crt%29%0A%20%20%20%20%20%20%20%20-%20**CREST**%u3010ICCV%2717%20Spotlight%u3011%5B%u3010CREST%3A%20Convolutional%20Residual%20Learning%20for%20Visual%20Tracking%u3011%5D%28http%3A//openaccess.thecvf.com/content_ICCV_2017/papers/Song_CREST_Convolutional_Residual_ICCV_2017_paper.pdf%29%5B%u3010GitHub%u3011%5D%28https%3A//github.com/ybsong00/CREST-Release%29%5B%u3010Project%u3011%5D%28http%3A//www.cs.cityu.edu.hk/%7Eyibisong/iccv17/index.html%29%0A%20%20%20%20%20%20%20%20-%20**UCT**%u3010ICCVW%2717%u3011%5B%u3010UCT%3A%20Learning%20Unified%20Convolutional%20Networks%20for%20Real-Time%20Visual%20Tracking%u3011%5D%28http%3A//openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w28/Zhu_UCT_Learning_Unified_ICCV_2017_paper.pdf%29%0A-%20**GOTURN**%u3010ECCV%2716%u3011%5B%u3010Learning%20to%20Track%20at%20100%20FPS%20with%20Deep%20Regression%20Networks%u3011%5D%28http%3A//davheld.github.io/GOTURN/GOTURN.pdf%29%5B%u3010GitHub%u3011%5D%28https%3A//github.com/davheld/GOTURN%29%5B%u3010Project%u3011%5D%28http%3A//davheld.github.io/GOTURN/GOTURN.html%29%0A%20%20%20%20-%20**DeepLK**%u3010ICRA%2718%u3011%5B%u3010Deep-LK%20for%20Efficient%20Adaptive%20Object%20Tracking%u3011%5D%28http%3A//ci2cv.net/media/papers/deepLK-icra.pdf%29%0A-%20**Learnet**%u3010NIPS%2716%u3011%5B%u3010Learning%20feed-forward%20one-shot%20learners%u3011%5D%28https%3A//arxiv.org/pdf/1606.05233v1.pdf%29%0A-%20**DRT**%u3010TIP%2717%u3011%5B%u3010Deep%20Relative%20Tracking%u3011%5D%28http%3A//ieeexplore.ieee.org/abstract/document/7828108/%29%0A-%20**Re3**%u30102017%u3011%5B%u3010Re3%20%3A%20Real-Time%20Recurrent%20Regression%20Networks%20for%20Object%20Tracking%u3011%5D%28https%3A//arxiv.org/pdf/1705.06368.pdf%29%0A-%20**SiameseFC**%u3010ECCV%2716%u3011%5B%u3010Fully-Convolutional%20Siamese%20Networks%20for%20Object%20Tracking%u3011%5D%28https%3A//arxiv.org/pdf/1606.09549.pdf%29%5B%u3010GitHub%u3011%5D%28https%3A//github.com/bertinetto/siamese-fc%29%5B%u3010Project%u3011%5D%28http%3A//www.robots.ox.ac.uk/%7Eluca/siamese-fc.html%29%0A%20%20%20%20-%20**SiamFC-tri**%u3010ECCV%2718%u3011%5B%u3010Triplet%20Loss%20in%20Siamese%20Network%20for%20Object%20Tracking%u3011%5D%28http%3A//openaccess.thecvf.com/content_ECCV_2018/papers/Xingping_Dong_Triplet_Loss_with_ECCV_2018_paper.pdf%29%5B%u3010GitHub%u3011%5D%28https%3A//github.com/shenjianbing/TripletTracking%29%0A%20%20%20%20-%20**MemTrack**%u3010ECCV%2718%u3011%5B%u3010Learning%20Dynamic%20Memory%20Networks%20for%20Object%20Tracking%u3011%5D%28http%3A//openaccess.thecvf.com/content_ECCV_2018/papers/Tianyu_Yang_Learning_Dynamic_Memory_ECCV_2018_paper.pdf%29%0A%20%20%20%20-%20**StructSiam**%u3010ECCV%2718%u3011%5B%u3010Structured%20Siamese%20Network%20for%20Real-Time%20Visual%20Tracking%u3011%5D%28http%3A//openaccess.thecvf.com/content_ECCV_2018/papers/Yunhua_Zhang_Structured_Siamese_Network_ECCV_2018_paper.pdf%29%0A%20%20%20%20-%20**HART**%u3010NIPS%2717%u3011%5B%u3010Hierarchical%20Attentive%20Recurrent%20Tracking%u3011%5D%28https%3A//papers.nips.cc/paper/6898-hierarchical-attentive-recurrent-tracking.pdf%29%5B%u3010GitHub%u3011%5D%28https%3A//github.com/akosiorek/hart%29%0A%20%20%20%20-%20**RASNet**%u3010CVPR%2718%u3011%5B%u3010Learning%20Attentions%3A%20Residual%20Attentional%20Siamese%20Network%20for%20High%20Performance%20Online%20Visual%20Tracking%u3011%5D%28http%3A//openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Learning_Attentions_Residual_CVPR_2018_paper.pdf%29%0A%20%20%20%20-%20**RFL**%u3010ICCVW%2717%u3011%5B%u3010Recurrent%20Filter%20Learning%20for%20Visual%20Tracking%u3011%5D%28http%3A//openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w28/Yang_Recurrent_Filter_Learning_ICCV_2017_paper.pdf%29%0A%20%20%20%20-%20**Dsiam**%u3010ICCV%2717%u3011%5B%u3010Learning%20Dynamic%20Siamese%20Network%20for%20Visual%20Object%20Tracking%u3011%5D%28http%3A//openaccess.thecvf.com/content_ICCV_2017/papers/Guo_Learning_Dynamic_Siamese_ICCV_2017_paper.pdf%29%5B%u3010GitHub%u3011%5D%28https%3A//github.com/tsingqguo/DSiam%29%0A%20%20%20%20-%20**EAST**%u3010ICCV%2717%u3011%5B%u3010Learning%20Policies%20for%20Adaptive%20Tracking%20with%20Deep%20Feature%20Cascades%u3011%5D%28http%3A//openaccess.thecvf.com/content_ICCV_2017/papers/Huang_Learning_Policies_for_ICCV_2017_paper.pdf%29%5B%u3010Supp%u3011%5D%28http%3A//openaccess.thecvf.com/content_ICCV_2017/supplemental/Huang_Learning_Policies_for_ICCV_2017_supplemental.zip%29%0A%20%20%20%20-%20**SA-Siam**%u3010CVPR%2718%u3011%5B%u3010A%20Twofold%20Siamese%20Network%20for%20Real-Time%20Object%20Tracking%u3011%5D%28http%3A//openaccess.thecvf.com/content_cvpr_2018/papers/He_A_Twofold_Siamese_CVPR_2018_paper.pdf%29%5B%u3010GitHub%u3011%5D%28https%3A//github.com/Microsoft/SA-Siam%29%0A%20%20%20%20%20%20%20%20-%20**SiamBM**%u30102018%u3011%5B%u3010Towards%20a%20Better%20Match%20in%20Siamese%20Network%20Based%20Visual%20Object%20Tracker%u3011%5D%28http%3A//staff.ustc.edu.cn/%7Exinmei/publications_pdf/2018/Towards%2520a%2520Better%2520Match%2520in%2520Siamese%2520Network%2520Based%2520Visual%2520Object%2520Tracker.pdf%29%0A%20%20%20%20-%20**CFNet**%u3010CVPR%2717%u3011%5B%u3010End-to-end%20representation%20learning%20for%20Correlation%20Filter%20based%20tracking%u3011%5D%28http%3A//openaccess.thecvf.com/content_cvpr_2017/papers/Valmadre_End-To-End_Representation_Learning_CVPR_2017_paper.pdf%29%5B%u3010Supp%u3011%5D%28http%3A//openaccess.thecvf.com/content_cvpr_2017/supplemental/Valmadre_End-To-End_Representation_Learning_2017_CVPR_supplemental.pdf%29%5B%u3010GitHub%u3011%5D%28https%3A//github.com/bertinetto/cfnet%29%5B%u3010Project%u3011%5D%28http%3A//www.robots.ox.ac.uk/%7Eluca/cfnet.html%29%0A%20%20%20%20%20%20%20%20-%20**RTINet**%u3010ECCV%2718%u3011%5B%u3010Joint%20Representation%20and%20Truncated%20Inference%20Learning%20for%20Correlation%20Filter%20based%20Tracking%u3011%5D%28http%3A//openaccess.thecvf.com/content_ECCV_2018/papers/Yingjie_Yao_Joint_Representation_and_ECCV_2018_paper.pdf%29%0A%20%20%20%20-%20**DCFNet**%u30102017%u3011%5B%u3010DCFNet%3A%20Discriminant%20Correlation%20Filters%20Network%20for%20Visual%20Tracking%u3011%5D%28https%3A//arxiv.org/pdf/1704.04057.pdf%29%5B%u3010GitHub%u3011%5D%28https%3A//github.com/foolwood/DCFNet%23dcfnet-discriminant-correlation-filters-network-for-visual-tracking%29%0A%20%20%20%20%20%20%20%20-%20**SACF**%u3010ECCV%2718%u3011%5B%u3010Visual%20Tracking%20via%20Spatially%20Aligned%20Correlation%20Filters%20Network%u3011%5D%28http%3A//openaccess.thecvf.com/content_ECCV_2018/papers/mengdan_zhang_Visual_Tracking_via_ECCV_2018_paper.pdf%29%0A%20%20%20%20%20%20%20%20-%20**FlowTrack**%u3010CVPR%2718%u3011%5B%u3010End-to-end%20Flow%20Correlation%20Tracking%20with%20Spatial-temporal%20Attention%u3011%5D%28http%3A//openaccess.thecvf.com/content_cvpr_2018/papers/Zhu_End-to-End_Flow_Correlation_CVPR_2018_paper.pdf%29%0A%20%20%20%20%20%20%20%20-%20**DSNet**%u30102018%u3011%5B%u3010DSNet%20for%20Real-Time%20Driving%20Scene%20Semantic%20Segmentation%u3011%5D%28https%3A//arxiv.org/abs/1812.07049%29%5B%u3010GitHub%u3011%5D%28https%3A//github.com/s7ev3n/DSNet%29%0A%20%20%20%20-%20**SiamRPN**%u3010CVPR%2718%20Spotlight%u3011%5B%u3010High%20Performance%20Visual%20Tracking%20with%20Siamese%20Region%20Proposal%20Network%u3011%5D%28http%3A//openaccess.thecvf.com/content_cvpr_2018/papers/Li_High_Performance_Visual_CVPR_2018_paper.pdf%29%5B%u3010GitHub%u3011%5D%28https%3A//github.com/arbitularov/SiamRPN-PyTorch%29%0A%20%20%20%20%20%20%20%20-%20**DaSiamRPN**%u3010ECCV%2718%u3011%5B%u3010Distractor-aware%20Siamese%20Networks%20for%20Visual%20Object%20Tracking%u3011%5D%28http%3A//openaccess.thecvf.com/content_ECCV_2018/papers/Zheng_Zhu_Distractor-aware_Siamese_Networks_ECCV_2018_paper.pdf%29%5B%u3010GitHub%u3011%5D%28https%3A//github.com/foolwood/DaSiamRPN%29%0A%20%20%20%20%20%20%20%20%20%20%20%20-%20**SiamMask**%u3010CVPR%2719%u3011%5B%u3010Fast%20Online%20Object%20Tracking%20and%20Segmentation%3A%20A%20Unifying%20Approach%u3011%5D%28https%3A//arxiv.org/pdf/1812.05050.pdf%29%5B%u3010GitHub%u3011%5D%28https%3A//github.com/foolwood/SiamMask%29%5B%u3010Project%u3011%5D%28http%3A//www.robots.ox.ac.uk/%7Eqwang/SiamMask/%29%0A%20%20%20%20%20%20%20%20%20%20%20%20-%20**SiamRPN++**%u3010CVPR%2719%20Oral%u3011%5B%u3010SiamRPN++%3A%20Evolution%20of%20Siamese%20Visual%20Tracking%20with%20Very%20Deep%20Networks%u3011%5D%28https%3A//arxiv.org/pdf/1812.11703.pdf%29%5B%u3010Project%u3011%5D%28http%3A//bo-li.info/SiamRPN++/%29%0A%20%20%20%20%20%20%20%20%20%20%20%20-%20**Siamese%20Cascade-RPN**%u3010CVPR%2719%u3011%5B%u3010Siamese%20Cascaded%20Region%20Proposal%20Networks%20for%20Real-Time%20Visual%20Tracking%u3011%5D%28http%3A//openaccess.thecvf.com/content_CVPR_2019/papers/Fan_Siamese_Cascaded_Region_Proposal_Networks_for_Real-Time_Visual_Tracking_CVPR_2019_paper.pdf%29%0A%20%20%20%20%20%20%20%20%20%20%20%20-%20**SPM**%u3010CVPR%2719%u3011%5B%u3010SPM-Tracker%3A%20Series-Parallel%20Matching%20for%20Real-Time%20Visual%20Object%20Tracking%u3011%5D%28https%3A//arxiv.org/pdf/1904.04452.pdf%29</center><br/></span>
</div></body></html> 